{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2d02526",
   "metadata": {},
   "source": [
    "### Dataframes, Spark SQL e Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c369a1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temos varias formas de crear un DataFrame:\n",
    "# - a partir dunha lista de datos\n",
    "# - lectura de ficheiros (diferentes formatos)\n",
    "# -- Local Filesystem\n",
    "# -- HDFS\n",
    "# -- nube: S3 Azure, HBase, Mysql ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6d89ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9e20c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df. show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bb1200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unha das novidades de Spark SQL é que permite facer consultas sobre as táboas/Dataframees como se\n",
    "# fosen táboas dunha base de datos relacional, utilizando ANSI SQL\n",
    "\n",
    "# O primeiro que hai que facer é crear unha táboa temporal sobre o Dataframe con 'createOrReplaceTempView()'\n",
    "# A continuación poderanse executar sentencias SQL a través da función 'sql()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79135f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('employees')\n",
    "spark.sql('SELECT * FROM employees').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309b6c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos gardar o resultado como un novo dataframe\n",
    "resultado = spark.sql('SELECT * FROM employees')\n",
    "resultado.printSchema()\n",
    "resultado.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e259247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos filtrar con condicións\n",
    "spark.sql(\"SELECT firstname, middlename, salary FROM employees WHERE salary > 3500\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d38594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos realizar funcións de agrupación\n",
    "spark.sql(\"SELECT gender, count(*) FROM employees GROUP BY gender\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3351dbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En resumo, spark.sql permítenos traballar cos Dataframes como se de táboas relacionais se tratase\n",
    "# e utilizar SQL para realizar as consultas que precisemos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3116bfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ademais dos ficheiros de texto CSV, tsv, xml, json... Spark pode traballar con outros formatos\n",
    "# Algúns formatos moi empregados son Avro ou Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7b1b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apache Parquet é un formato de almacenamento con almacenamento columnar. Esta característica fai\n",
    "# que sexa moi rápido no procesado de consultas de agregación\n",
    "# PySpark soporta Parquet de xeito nativo, sen necesidade de icorporar novas librarías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea43a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escritura a formato parquet\n",
    "df.write.parquet('file:///tmp/employees.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a492fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de format parquet\n",
    "df_parquet = spark.read.parquet('file:///tmp/employees.parquet')\n",
    "df_parquet.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737e0bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unhas das particularidades de Parquet é que permite crear particións, de xeito que os datos\n",
    "# se poden almacenar en ficheiros separados en función das nosas necesidades, o que pode mellorar\n",
    "# o rendemento en certas consultas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c904c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos crear particións para os datos, separando os datos por 'gender' e 'salary' en ficheiros diferentes\n",
    "df.write.partitionBy(\"gender\",\"salary\").mode(\"overwrite\").parquet(\"file:///tmp/output/employees2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d58b389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos cargar o ficheiro completo\n",
    "df_parquet_partido = spark.read.parquet(\"file:///tmp/output/employees2.parquet\")\n",
    "df_parquet_partido.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fb383a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos cargar só unha partición (a partición de )gender=M)\n",
    "df_parquet_partido = spark.read.parquet(\"file:///tmp/output/employees2.parquet/gender=M\")\n",
    "df_parquet_partido.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f9fd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos cargar só unha partición máis específica\n",
    "# Fíxate como xa non aparecen os campos\n",
    "df_parquet_partido = spark.read.parquet(\"file:///tmp/output/employees2.parquet/gender=M/salary=4000\")\n",
    "df_parquet_partido.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6614d5ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec70eca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
